{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e8c35d9",
   "metadata": {},
   "source": [
    "\"ld. The thredhold of eye blink is 80% of the average maximum. Also\n",
    "while running program the baseline will be newly calculated every\n",
    "3 epochs (6 seconds) to adjust a new threshold within limited boundaries.\n",
    "After that the system also calculated the energy of eye behavior in each\n",
    "subject to detect eye blinks and eye movements. If the signal is in the\n",
    "range of eye behavior, eye blinks and eye movements will perform. If the\n",
    "signal exceeds a range of eye behavior, artifacts will perform. Thereby,\n",
    "our systems can detect eye behavior out of artifacts and use eye behavior\n",
    "to detect state of drowsiness.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae34333e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in /opt/anaconda3/lib/python3.8/site-packages (1.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b957cc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Tuple, Union, Optional, Callable, List\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "from torchsummary import summary\n",
    "from torchmetrics import MetricCollection, Accuracy, Precision, Recall, F1Score, AUROC\n",
    "#from mind_ml.models.EEGNet import EEGNetLightning\n",
    "\n",
    "from torchtyping import TensorType, patch_typeguard\n",
    "#from typeguard import typechecked\n",
    "\n",
    "#patch_typeguard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e491a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.eneuro.org/content/9/5/ENEURO.0160-22.2022\n",
    "    https://github.com/yoshidan/pytorch-eyeblink-detection\n",
    "        https://hal.science/hal-01917529/document\n",
    "            https://arxiv.org/pdf/2101.10932.pdf\n",
    "                https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10079879/\n",
    "                    https://github.com/berdakh/eeg-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ca7dcb",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22c76f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module): #pl.LightningModule\n",
    "    \"\"\"\n",
    "    CNN model for EOG data.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 channels: int,\n",
    "                 dropout: float,\n",
    "                 kernel_size: int,\n",
    "                 sample_rate: int = 256,\n",
    "                 chan_out: int = 256,\n",
    "                 pool_out: int = 120,\n",
    "                 n_blocks: int = 3,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        channels : int, Number of EEG channels.\n",
    "        dropout : float\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.channels = channels\n",
    "        self.sample_rate = sample_rate\n",
    "        self.dropout = dropout\n",
    "        self.kernel_size = kernel_size\n",
    "        self.chan_out = chan_out\n",
    "        self.pool_out = pool_out\n",
    "        self.n_blocks = n_blocks\n",
    "\n",
    "        \"\"\"\n",
    "        TO DO:\n",
    "        \"\"\"\n",
    "\n",
    "        self.blocks = dict()\n",
    "        # initialize empty dictionary 'blocks' as attribute of current object('self')\n",
    "        for n in range(n_blocks):\n",
    "            # Add new key-value pair to 'blocks' dictionary\n",
    "            self.blocks[f\"block_{n}\"] = nn.Sequential( #container for sequence of layers\n",
    "                #1D convolution operation on input \n",
    "                nn.Conv1d(in_channels=channels, #number of input channels\n",
    "                        out_channels=chan_out, #number of output channels\n",
    "                        kernel_size=kernel_size, #size of convolutional kernel\n",
    "                        padding=\"same\", #padding mode to apply\n",
    "                        ),\n",
    "                nn.LazyBatchNorm1d(), #Performs batch normalization on input, normalize input along channel dimension \n",
    "                # helps stabilizing and accelerating training of neural networks. Improve convergence, generalization, and reduce overfitting \n",
    "                # makes network less sensitive to the scale of input features and helps with gradient flow during backpropagation\n",
    "                nn.GELU(), #Applies GELU activation function to the input (smooth approximation of rectified linear unit activation function)\n",
    "                nn.AdaptiveMaxPool1d(output_size = pool_out), #Layer applies adaptive max pooling to input \n",
    "                # Adapts input to have specified output size by performing max pooling \n",
    "                nn.Dropout(p=dropout), #randomly sets elements of the input tensor to zero with probability p, p = dropout value \n",
    "            )\n",
    "            channels = chan_out\n",
    "            chan_out = chan_out//2 #rounds down to nearest integer\n",
    "            pool_out = pool_out//2\n",
    "\n",
    "        #This is just to make it run with EEGNetLightning\n",
    "        self.conv_net = nn.Sequential(*[ #* unpacks list of blocks to be passed as individual arguments to nn.Sequential() \n",
    "            #creates new attribute \n",
    "            #sequence of blocks from self.blocks.values() \n",
    "            block for block in self.blocks.values() \n",
    "            #self.blocks contains blocks of convolutional network\n",
    "        ])\n",
    "        self.classifier_head = nn.Sequential(*[\n",
    "            nn.Flatten(), #layer that flattens multi-dimensional input tensor into 1D tensor. \n",
    "            # converts output of convolutional layers into flat feature vector, which can be passed to fully connected layers\n",
    "            # result: channels * pixels (row) * pixels (column)\n",
    "            nn.LazyLinear(out_features = 2), #2 output features/classes, not initialized until accessed first time\n",
    "            nn.GELU()#Gaussian Error Linear Unit activation function, introducing non-linearity into the model and helps capture complex relationships between features\n",
    "            ])\n",
    "\n",
    "    def forward(self, x: TensorType[\"num_batches\", \"num_channels\", \"num_samples\"]) -> TensorType[\"num_batches\", \"kernal_size\", \"reduced_channels\", 1]:\n",
    "        # for n in range(self.n_blocks):\n",
    "        #     x = self.blocks[f\"block_{n}\"](x)\n",
    "        x = self.conv_net(x) #instance of nn.Sequential wiht blocks of convolutional network\n",
    "        #Input tensor x goes through sequence of convolutional, normalization, activation, pooling, and dropout operatons defined in the blocks \n",
    "        x = self.classifier_head(x)\n",
    "        # Applies classifier head part of the model to the output of the convolutional netowkr \n",
    "        #self.classifier_head is instance of nn.Sequential containing layers responsible for classification \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca83020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Tuple, Union, Optional, Callable, List\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "from torchsummary import summary\n",
    "from torchmetrics import MetricCollection, Accuracy, Precision, Recall, F1Score, AUROC\n",
    "\n",
    "from torchtyping import TensorType, patch_typeguard\n",
    "from typeguard import typechecked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776f2314",
   "metadata": {},
   "source": [
    "## DepthwiseConv2D, SeparableConv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b7c032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras offers a DepthwiseConv2D layer as well as a SeparableConv2D layer.\n",
    "# The DepthwiseConv2D layer performs a depthwise convolution that acts separately on channels,\n",
    "# while the SeparableConv2D performs a depthwise convolution that acts separately on channels, followed by a pointwise convolution that mixes channels.\n",
    "# The pytorch equivalent is as follows:\n",
    "class DepthwiseConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Each input channel is convolved separately with its own set of filters\n",
    "    \n",
    "    From the documentation of torch.nn.Conv2d:\n",
    "    If groups == in_channels and out_channels == K * in_channels, where K is a positive integer,\n",
    "    this operation is also known as a depthwise convolution.\n",
    "    In other words, for an input of size (N, C_in, L_in), a depthwise convolution with a depthwise multiplier K,\n",
    "    can be constructed by providing the arguments (C_in = C_in, C_out = C_in * K, ..., groups = C_in).\n",
    "    \n",
    "    depth_multiplier (K): determines number of output channels for each input channel \n",
    "    Total number of output channels = in_channels * depth_multiplier\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, depth_multiplier, **kwargs):\n",
    "        super(DepthwiseConv2d, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels=in_channels,\n",
    "                                   out_channels=in_channels * depth_multiplier,\n",
    "                                   groups=in_channels, # each input channel will be convolved separately with its own set of filters\n",
    "                                   **kwargs) #passed to nn.Conv2d constructor\n",
    "\n",
    "    def forward(self, x): #implements forward pass of DepthwiseConv2d module\n",
    "        out = self.depthwise(x) \n",
    "        return out\n",
    "\n",
    "\n",
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, depth_multiplier=1, **kwargs):\n",
    "        super(SeparableConv2d, self).__init__()\n",
    "        self.depthwise = DepthwiseConv2d(in_channels, depth_multiplier, **kwargs)\n",
    "        self.pointwise = nn.Conv2d(in_channels=in_channels * depth_multiplier,\n",
    "                                   out_channels=out_channels,\n",
    "                                   kernel_size=(1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0d36eb",
   "metadata": {},
   "source": [
    "## EEGNet Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2dceed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGNetBackbone(nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch implementation of the EEGNet's backbone (convnet) from Lawhern et al. 2018.\n",
    "\n",
    "    Reference Implementation of EEGNet Version 3 from original authors:\n",
    "    https://github.com/vlawhern/arl-eegmodels/blob/master/EEGModels.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 channels= 1,\n",
    "                 sample_length= 256,\n",
    "                 kernel_size: int = 64,\n",
    "                 dropout: float = 0.0,\n",
    "                 f1: int = 8,\n",
    "                 d: int = 2,\n",
    "                 f2: int = 16,\n",
    "                 sample_rate: int = 256,\n",
    "                 adjust_for_sample_length: bool = False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        channels : int\n",
    "            Number of channels in the input data.\n",
    "        sample_rate : int\n",
    "            Sample rate of the input data, the architecture is designed for 256 Hz, but should work for higher sample rates.\n",
    "        dropout : float\n",
    "            Dropout rate.\n",
    "        kernel_size : int\n",
    "            Length of the temporal convolution kernal in the first layer.\n",
    "        f1, f2 : int\n",
    "            Number of temporal filters (F1) and number of pointwise filters (F2) to learn.\n",
    "            Default: F1 = 8, F2 = F1 * D\n",
    "        d : int\n",
    "            Number of spatial filters to learn within each temporal convolution.\n",
    "            Default: d = 2\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.channels = channels\n",
    "        self.sample_rate = sample_rate\n",
    "        self.sample_length = sample_length\n",
    "        self.dropout = dropout\n",
    "        self.kernel_size = kernel_size\n",
    "        self.f1 = f1\n",
    "        self.d = d\n",
    "        self.f2 = f2\n",
    "\n",
    "        # The authors describe their updated model as follows:\n",
    "        # There are two CNN blocks followed by a fully connected layer.\n",
    "        # Block 1:\n",
    "        #  - Vanilla 2D Convolution with same padding and kernal size (1, kernel_size)\n",
    "        #  - Batch normalization\n",
    "        #  - Depthwise Convolution with kernal size (channels, 1) and depth multiplier d\n",
    "        #  - Batch normalization\n",
    "        #  - ELU activation\n",
    "        #  - Average pooling with kernal size (1, 4)\n",
    "        #  - Dropout or Spatial Dropout\n",
    "        # Block 2:\n",
    "        #  - Depthwise Separable Convolution with output channels f2, kernal size (1, 16) and same padding\n",
    "        #  - Batch normalization\n",
    "        #  - ELU activation\n",
    "        #  - Average pooling with kernal size (1, 8)\n",
    "        #  - Dropout or Spatial Dropout\n",
    "        # Flatten\n",
    "\n",
    "        \"\"\"\n",
    "        The following is the orignal implementation using keras\n",
    "\n",
    "\n",
    "        input1       = Input(shape = (Chans, Samples, 1))\n",
    "        block1       = Conv2D(F1, (1, kernLength), padding = 'same',\n",
    "                                    input_shape = (Chans, Samples, 1),\n",
    "                                    use_bias = False)(input1)\n",
    "        block1       = BatchNormalization()(block1)\n",
    "        block1       = DepthwiseConv2D((Chans, 1), use_bias = False,\n",
    "                                    depth_multiplier = D,\n",
    "                                    depthwise_constraint = max_norm(1.))(block1)\n",
    "        block1       = BatchNormalization()(block1)\n",
    "        block1       = Activation('elu')(block1)\n",
    "        block1       = AveragePooling2D((1, 4))(block1)\n",
    "        block1       = dropoutType(dropoutRate)(block1)\n",
    "\n",
    "        block2       = SeparableConv2D(F2, (1, 16),\n",
    "                                    use_bias = False, padding = 'same')(block1)\n",
    "        block2       = BatchNormalization()(block2)\n",
    "        block2       = Activation('elu')(block2)\n",
    "        block2       = AveragePooling2D((1, 8))(block2)\n",
    "        block2       = dropoutType(dropoutRate)(block2)\n",
    "\n",
    "        flatten      = Flatten(name = 'flatten')(block2)\n",
    "        \"\"\"\n",
    "\n",
    "        # We want the pytorch equivalent of the keras implementation above\n",
    "        # Note that the input shape for pytorch is (Batch, Channels, Height, Width)\n",
    "        # for our case height will be the spatial dimension and width will be the temporal dimension\n",
    "        # spatial dimension is the number of recording channels (not to be confused with \"channels\" from the pytorch perspective)\n",
    "        # temporal dimension is the number of samples\n",
    "        self.pool1 = 4\n",
    "        # Block 1\n",
    "        self.block1 = nn.Sequential(\n",
    "            # shape (batch, 1, channels, samples)\n",
    "            # first a temporal convolution with kernel size (1, kernel_size) ignoring the spatial dimension\n",
    "            nn.Conv2d(in_channels=1,\n",
    "                      out_channels=f1,\n",
    "                      kernel_size=(1, kernel_size),\n",
    "                      padding=\"same\",\n",
    "                      bias=False),\n",
    "            # shape (batch, f1, channels, samples)\n",
    "            nn.BatchNorm2d(num_features=f1),\n",
    "            # shape (batch, f1, channels, samples)\n",
    "            # next a depthwise convolution over the spatial dimension to learn frequency specific spatial filters\n",
    "            DepthwiseConv2d(in_channels=f1,\n",
    "                            depth_multiplier=d,\n",
    "                            kernel_size=(channels, 1),\n",
    "                            padding=\"valid\",\n",
    "                            bias=False),\n",
    "            # shape (batch, f1 * d, 1, samples)\n",
    "            nn.BatchNorm2d(num_features=f1 * d),\n",
    "            # shape (batch, f1 * d, 1, samples)\n",
    "            nn.ELU(),\n",
    "            # shape (batch, f1 * d, 1, samples)\n",
    "            # the pooling is done over the temporal dimension\n",
    "            nn.AvgPool2d(kernel_size=(1, self.pool1)),\n",
    "            # shape (batch, f1 * d, 1, samples // 4)\n",
    "            nn.Dropout(p=dropout),\n",
    "            # shape (batch, f1 * d, 1, samples // 4)\n",
    "        )\n",
    "\n",
    "        # in previous versions the final pooling was hard cored to 8\n",
    "        # assuming a sample rate of 256 Hz\n",
    "        self.pool2 = 8\n",
    "\n",
    "        # but actually if we increase the sample length (more than one second of data)\n",
    "        # we would want to pool more aggressively to reduce the output size to the same dimension\n",
    "        # otherwise the fully connected layer will need to be very large\n",
    "        # we can adjust the pooling size based on ratio of sample length and sample rate\n",
    "        if adjust_for_sample_length:\n",
    "            ratio = sample_length / sample_rate\n",
    "            self.pool2 = int(self.pool2 * ratio)\n",
    "\n",
    "        # Block 2\n",
    "        self.block2 = nn.Sequential(\n",
    "            # shape (batch, f1 * d, 1, samples // 4)\n",
    "            # again a temporal convolution but this time as a depthwise separable convolution\n",
    "            SeparableConv2d(in_channels=f1 * d,\n",
    "                            out_channels=f2,\n",
    "                            depth_multiplier=1,\n",
    "                            kernel_size=(1, 16),\n",
    "                            padding=\"same\",\n",
    "                            bias=False),\n",
    "            # shape (batch, f2, samples / 4, 1)\n",
    "            nn.BatchNorm2d(num_features=f2),\n",
    "            # shape (batch, f2, samples / 4, 1)\n",
    "            nn.ELU(),\n",
    "            # shape (batch, f2, samples / 4, 1)\n",
    "            # the pooling is done over the temporal dimension\n",
    "            nn.AvgPool2d(kernel_size=(1, self.pool2)),\n",
    "            # shape (batch, f2, samples / 32, 1)\n",
    "            nn.Dropout(p=dropout),\n",
    "            # shape (batch, f2, samples / 32, 1)\n",
    "        )\n",
    "\n",
    "        self.output_shape = (f2, sample_length // (self.pool1*self.pool2), 1)\n",
    "\n",
    "    def forward(self, x: TensorType[\"num_batches\", 1, \"num_channels\", \"num_samples\"]) -> TensorType[\"num_batches\", \"kernal_size\", \"reduced_channels\", 1]:\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80d5f6c",
   "metadata": {},
   "source": [
    "## EEGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963deb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch implementation of the EEGNet model from Lawhern et al. 2018.\n",
    "\n",
    "    Reference Implementation of EEGNet Version 3 from original authors:\n",
    "    https://github.com/vlawhern/arl-eegmodels/blob/master/EEGModels.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_classes= 2, #number of classes to predict \n",
    "                 classifier_hidden_units: Optional[int] = None, #number of hidden units(neurons) in each hidden layer of classifer\n",
    "                 classifier_num_layers: int = 1, #hidden layers in the classifier\n",
    "                 **backbone_kwargs): \n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_classes : int\n",
    "            Number of classes to predict\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        # Calls constructor of parent class nn.Module to initialize EEGNet module\n",
    "        self.classifier_hidden_units = classifier_hidden_units\n",
    "        self.classifier_num_layers = classifier_num_layers\n",
    "        self.classifier_num_hidden_layers = classifier_num_layers - 1\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Ensures that if multiple hidden layers are specified, the number of hidden units is also provided\n",
    "        assert self.classifier_num_hidden_layers == 0 or self.classifier_hidden_units is not None, \"If classifier_num_layers > 1, classifier_hidden_units must be specified\"\n",
    "        \"\"\"\n",
    "        conv_net = ...\n",
    "        flatten      = Flatten(name = 'flatten')(conv_net)\n",
    "\n",
    "        dense        = Dense(nb_classes, name = 'dense',\n",
    "                            kernel_constraint = max_norm(norm_rate))(flatten)\n",
    "        softmax      = Activation('softmax', name = 'softmax')(dense)\n",
    "        return Model(inputs=input1, outputs=softmax)\n",
    "        \"\"\"\n",
    "        # for backwards compatibility with previous versions of this EEGNet implementation we need to use nn.Sequential\n",
    "        # otherwise we couldn't use the pretrained weights / re-evaulate older models\n",
    "        # self.conv_net = EEGNetBackbone(**backbone_kwargs)\n",
    "        backbone = EEGNetBackbone(**backbone_kwargs)\n",
    "        self.dropout = backbone.dropout\n",
    "\n",
    "        self.conv_net = nn.Sequential(*[\n",
    "            backbone.block1,\n",
    "            backbone.block2,\n",
    "        ])\n",
    "        \n",
    "        # Calculate the input units, output units, and total number of input units for classifier head\n",
    "        # output shape of conv_net is (batch, f2, samples / 32, 1)\n",
    "        classifier_input_units = np.array(backbone.output_shape).prod()\n",
    "        input_units = [classifier_input_units] + [classifier_hidden_units] * self.classifier_num_hidden_layers\n",
    "        output_units = [classifier_hidden_units] * self.classifier_num_hidden_layers + [num_classes]\n",
    "\n",
    "        # Layers stored in list called 'classifier_layers'\n",
    "        classifier_layers = [nn.Flatten()] # Flatten input tensor\n",
    "        for in_features, out_features in zip(input_units, output_units): \n",
    "            # Each nn.Linear layer has in_features input units and out_features output units with nn.ELU activation function\n",
    "            classifier_layers.append(nn.Linear(in_features=in_features, out_features=out_features))\n",
    "            classifier_layers.append(nn.ELU())\n",
    "\n",
    "        # remove the last activation layer\n",
    "        classifier_layers = classifier_layers[:-1]\n",
    "\n",
    "        # creates sequential module\n",
    "        self.classifier_head = nn.Sequential(*classifier_layers)\n",
    "        # output shape of classifier_head is (batch, num_classes)\n",
    "\n",
    "    def forward(self, x: TensorType[\"num_batches\", 1, \"num_channels\", \"num_samples\"]) -> TensorType[\"num_batches\", \"num_classes\"]:\n",
    "        x = self.conv_net(x)\n",
    "        x = self.classifier_head(x)\n",
    "        return x\n",
    "\n",
    "    # Used for feature embedding. Takes input tensor x and performs convolutional operations using self.conv_net\n",
    "    def embed(self, x: TensorType[\"num_batches\", 1, \"num_channels\", \"num_samples\"]) -> TensorType[\"num_batches\", \"classifier_input_units\"]:\n",
    "        x = self.conv_net(x)\n",
    "        x = x.view(x.shape[0], -1) #shape(num_batches, classifier_input_units)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518d599b",
   "metadata": {},
   "source": [
    "## Conditional EEGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea3fe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalEEGNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A model based on EEGNet but with the ability to condition it's prediction for any given sample on samples from a calibration sequence\n",
    "\n",
    "    The architecture changes as follows:\n",
    "    - the input is now a tuple (x_calib, x_sample) where the shapes are\n",
    "        - x_calib: (num_batches, num_calib_samples, 1, num_channels, num_samples)\n",
    "        - x_sample: (num_batches, 1, num_channels, num_samples)\n",
    "        Note: for technical reasons the input to the forward method is a single tensor with shape (num_batches, num_sequences, 1, num_channels, num_samples)\n",
    "        where num_sequences = num_calib_samples + 1\n",
    "    - both x_calib and x_sample are passed through the EEGNet backbone\n",
    "    - the outputs for x_calib are aggregated (various aggregation methods are possible)\n",
    "    - the output for x_sample is then aggregated with the x_calib_aggregated (again various aggregation methods are possible)\n",
    "    - the aggregated output is passed through a classifier head\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_classes: int,\n",
    "                 calibration_aggregation_method: str, #aggregate outputs of EEGNet backbone for calibration sequence\n",
    "                 pre_classifier_aggregation_method: str, #aggregate outputs of EEGNet backbone for sample and aggregated calibration sequence\n",
    "                 classifier_hidden_units: Optional[int] = None,\n",
    "                 classifier_num_layers: int = 1,\n",
    "                 **backbone_kwargs):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_classes : int\n",
    "            Number of classes to predict\n",
    "        calibration_aggregation_method : str\n",
    "            The method used to aggregate the outputs of the EEGNet backbone for the calibration sequence\n",
    "        pre_classifier_aggregation_method : str\n",
    "            The method used to aggregate the outputs of the EEGNet backbone for the sample and the aggregated calibration sequence\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.calibration_aggregation_method = calibration_aggregation_method\n",
    "        self.pre_classifier_aggregation_method = pre_classifier_aggregation_method\n",
    "\n",
    "        self.classifier_hidden_units = classifier_hidden_units\n",
    "        self.classifier_num_layers = classifier_num_layers\n",
    "        self.classifier_num_hidden_layers = classifier_num_layers - 1\n",
    "        self.num_classes = num_classes\n",
    "        # ensure that if multiple hidden layers are specified, number of hidden units is also provided\n",
    "        assert self.classifier_num_hidden_layers == 0 or self.classifier_hidden_units is not None, \"If classifier_num_layers > 1, classifier_hidden_units must be specified\"\n",
    "\n",
    "        self.conv_net = EEGNetBackbone(**backbone_kwargs)\n",
    "        # output shape is (batch, f2, samples / 32, 1)\n",
    "        conv_net_output_shape = self.conv_net.output_shape\n",
    "\n",
    "        # two times the backbone output shape\n",
    "        # because we concatenate the aggregated calibration sequence with the sample's output\n",
    "        # classifier input shape might differ depending on pre_classifier_aggregation_method\n",
    "        if pre_classifier_aggregation_method == \"concat\":\n",
    "            classifier_input_shape = (2, *conv_net_output_shape)\n",
    "        elif pre_classifier_aggregation_method in [\"mean\", \"max\", \"min\"]:\n",
    "            classifier_input_shape = (1, *conv_net_output_shape)\n",
    "        elif \"difference\" in pre_classifier_aggregation_method:\n",
    "            classifier_input_shape = (1, *conv_net_output_shape)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pre_classifier_aggregation_method: {pre_classifier_aggregation_method}\")\n",
    "\n",
    "        classifier_input_units = np.array(classifier_input_shape).prod() \n",
    "        #compute product of elements in classifier_input_shape to obtain total number of units in input tensor for classifier head\n",
    "        input_units = [classifier_input_units] + [classifier_hidden_units] * self.classifier_num_hidden_layers\n",
    "        output_units = [classifier_hidden_units] * self.classifier_num_hidden_layers + [num_classes]\n",
    "\n",
    "        classifier_layers = [nn.Flatten(start_dim=-4)]\n",
    "        for in_features, out_features in zip(input_units, output_units): #number of input and output units, for each layer of classifier\n",
    "            classifier_layers.append(nn.Linear(in_features=in_features, out_features=out_features))\n",
    "            classifier_layers.append(nn.ELU()) #Exponent Linear Unit applies element-wise non-linearity to the output of previous linear layer\n",
    "\n",
    "        # remove the last activation layer\n",
    "        # Ensures that final layer doesn't have additional activation function, as model's forward pass is expected to reutrn logits before applying the final activation (softmax) for classification \n",
    "        classifier_layers = classifier_layers[:-1]\n",
    "\n",
    "        self.classifier_head = nn.Sequential(*classifier_layers)\n",
    "        # output shape of classifier_head is (batch, num_classes)\n",
    "\n",
    "    def forward(self, x: TensorType[\"num_batches\", \"num_sequences\", 1, \"num_channels\", \"num_samples\"]) -> TensorType[\"num_batches\", \"num_classes\"]:\n",
    "        # first pass all samples through the conv_net backbone\n",
    "        # the conv_net backbone operates on 4D (batched) tensors so we need to flatten the first two dimensions and then reshape the output back to the original shape\n",
    "        num_batches, num_sequences = x.shape[:2] #extract the sizes of the batch and sequence dimensions from the shape of x\n",
    "        #x.view to reshape tensor\n",
    "        x = x.view(num_batches * num_sequences, *x.shape[2:])\n",
    "        x = self.conv_net(x)\n",
    "        x = x.view(num_batches, num_sequences, *x.shape[1:])\n",
    "\n",
    "        # split the input tensor into calibration and sample tensors\n",
    "        x_calib, x_sample = x[:, :-1], x[:, -1:]\n",
    "\n",
    "        # aggregate the outputs of the conv_net backbone for the calibration sequence\n",
    "        # Condition prediction for given sample on samples from calibration sequence (a reference or context for making predictions)\n",
    "        # Calibration sequence: set of samples or data points used to calibrate model or estimator to provide refernce of making prediction\n",
    "        # Model aggregates outputs of EEGNet backbone for calibration sequence and combines with output for sample to make final prediction (prior knowledge for informed prediction)\n",
    "        if self.calibration_aggregation_method == \"mean\":\n",
    "            x_calib = x_calib.mean(dim=1).unsqueeze(1)\n",
    "        elif self.calibration_aggregation_method == \"max\":\n",
    "            x_calib = x_calib.max(dim=1)[0].unsqueeze(1)\n",
    "        elif self.calibration_aggregation_method == \"min\":\n",
    "            x_calib = x_calib.min(dim=1)[0].unsqueeze(1)\n",
    "        elif self.calibration_aggregation_method == \"none\":\n",
    "            # x_calib stays the same\n",
    "            # but for this case we need to expand the dimension of x_sample instead so that the concatenation below works\n",
    "            x_sample = x_sample.expand(x_calib.shape).unsqueeze(1)\n",
    "            x_calib = x_calib.unsqueeze(1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown calibration_aggregation_method: {self.calibration_aggregation_method}\")\n",
    "\n",
    "        # Aggregate the outputs of the conv_net backbone for the sample and the aggregated calibration sequence\n",
    "        x_pre_classifier = torch.cat([x_calib, x_sample], dim=1)\n",
    "        assert x_pre_classifier.shape[\n",
    "            1] == 2, f\"Expected x_pre_classifier to have shape (batch, 2, ...), but got {x_pre_classifier.shape}\"\n",
    "\n",
    "        # Since we operate over axis=1 where we will always have 2 elements, we can also use other aggregation methods here\n",
    "        # Perform aggregation on x_pre_classifier\n",
    "        if self.pre_classifier_aggregation_method == \"mean\":\n",
    "            x_pre_classifier = x_pre_classifier.mean(dim=1).unsqueeze(1)\n",
    "        elif self.pre_classifier_aggregation_method == \"max\":\n",
    "            x_pre_classifier = x_pre_classifier.max(dim=1)[0].unsqueeze(1)\n",
    "        elif self.pre_classifier_aggregation_method == \"min\":\n",
    "            x_pre_classifier = x_pre_classifier.min(dim=1)[0].unsqueeze(1)\n",
    "        elif self.pre_classifier_aggregation_method == \"difference\":\n",
    "            x_pre_classifier = (x_pre_classifier[:, 0] - x_pre_classifier[:, 1]).unsqueeze(1)\n",
    "        elif self.pre_classifier_aggregation_method == \"abs_difference\":\n",
    "            x_pre_classifier = torch.abs(x_pre_classifier[:, 0] - x_pre_classifier[:, 1]).unsqueeze(1)\n",
    "        elif self.pre_classifier_aggregation_method == \"square_difference\":\n",
    "            x_pre_classifier = torch.square(x_pre_classifier[:, 0] - x_pre_classifier[:, 1]).unsqueeze(1)\n",
    "        elif self.pre_classifier_aggregation_method == \"concat\":\n",
    "            x_pre_classifier = x_pre_classifier\n",
    "        # even various distance metrics are possible here\n",
    "        elif self.pre_classifier_aggregation_method == \"cosine_similarity\":\n",
    "            x_pre_classifier = F.cosine_similarity(x_pre_classifier[:, 0], x_pre_classifier[:, 1])\n",
    "        elif self.pre_classifier_aggregation_method == \"euclidean_distance\":\n",
    "            x_pre_classifier = F.pairwise_distance(x_pre_classifier[:, 0], x_pre_classifier[:, 1])\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pre_classifier_aggregation_method: {self.pre_classifier_aggregation_method}\")\n",
    "\n",
    "        # Reshape x_pre_classifier if calibration method is none\n",
    "        # TODO: This might be 6 dimensional in the case of \"none\" for the calibration_aggregation_method\n",
    "        assert len(x_pre_classifier.shape) >= 5, \\\n",
    "            f\"Expected x_pre_classifier to be at least 5-dimensional, but got {x_pre_classifier.shape}\"\n",
    "\n",
    "        if len(x_pre_classifier.shape) == 6:\n",
    "            # switch axis 1 and 2, since the last 4 dimensions are operated on by the classifier head\n",
    "            x_pre_classifier = x_pre_classifier.permute(0, 2, 1, 3, 4, 5)\n",
    "        # pass the aggregated output through the classifier head\n",
    "        x = self.classifier_head(x_pre_classifier)\n",
    "\n",
    "        if len(x.shape) == 3:\n",
    "            # average over the second dimension\n",
    "            x = x.mean(dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db97863",
   "metadata": {},
   "source": [
    "## EEGNetLightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b3d33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalEEGNetLightning(EEGNetLightning):\n",
    "    def __init__(self, **hparams):\n",
    "        super().__init__(**hparams)\n",
    "        self.eegnet = ConditionalEEGNet(**hparams)\n",
    "\n",
    "    def forward(self, x: TensorType[\"num_batches\", \"sequence_length\", \"num_samples\", \"num_channels\"]) -> TensorType[\"num_batches\", \"num_classes\"]:\n",
    "        # x is of shape (..., samples, channels) but we need (..., 1, channels, samples)\n",
    "        x = torch.swapaxes(x, -2, -1)\n",
    "        x = x.unsqueeze(-3)\n",
    "        y_hat = self.eegnet(x)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d75f4d",
   "metadata": {},
   "source": [
    "## CNN Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b90926",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLightning(EEGNetLightning):\n",
    "    def __init__(self, **hparams):\n",
    "        super().__init__(**hparams)\n",
    "        self.eegnet = CNN(**hparams)\n",
    "\n",
    "    def forward(self, x: TensorType[\"num_batches\", 1, \"num_channels\", \"num_samples\"]) -> TensorType[\"num_batches\", \"num_classes\"]:\n",
    "        # x is of shape (..., samples, channels) but we need (..., 1, channels, samples)\n",
    "        #x = torch.swapaxes(x, -2, -1)\n",
    "        #x = x.unsqueeze(-3)\n",
    "        x = x.squeeze(axis=1)\n",
    "        x = torch.swapaxes(x, -2, -1)\n",
    "        y_hat = self.eegnet(x)\n",
    "        return y_hat\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        assert hasattr(torch.optim, self.hparams.optimizer_class),\\\n",
    "            f\"{self.hparams.optimizer_class} is not a valid optimizer from torch.optim\"\n",
    "        optimizer_class = getattr(torch.optim, self.hparams.optimizer_class)\n",
    "        optimizer = optimizer_class(\n",
    "            [\n",
    "                {  # conv layer parameters\n",
    "                    \"params\": filter(lambda p: p.requires_grad, self.eegnet.conv_net.parameters()),\n",
    "                    \"weight_decay\": getattr(self.hparams, \"conv_weight_decay\", 0.0),\n",
    "                },\n",
    "                {  # fc layer parameters\n",
    "                    \"params\": filter(lambda p: p.requires_grad, self.eegnet.classifier_head.parameters()),\n",
    "                    \"weight_decay\": getattr(self.hparams, \"fc_weight_decay\", 0.0),\n",
    "                },\n",
    "            ],\n",
    "            lr=self.hparams.learning_rate)\n",
    "\n",
    "        # lr = torch.optim.lr_scheduler.CyclicLR(\n",
    "        #     optimizer, base_lr = self.hparams.learning_rate,\n",
    "        #     max_lr = 4*self.hparams.learning_rate,\n",
    "        #     step_size_up = 4*int(self.stepsize),\n",
    "        #     mode = \"triangular\",\n",
    "        #     cycle_momentum = False\n",
    "        #     )\n",
    "\n",
    "        lr = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer = optimizer,\n",
    "            max_lr = self.hparams.learning_rate,\n",
    "            epochs = self.trainer.max_epochs,\n",
    "            steps_per_epoch = self.trainer.estimated_stepping_batches // self.trainer.max_epochs,\n",
    "            cycle_momentum = True\n",
    "            )\n",
    "\n",
    "        scheduler = {\n",
    "            \"scheduler\": lr,\n",
    "            \"interval\": \"step\",\n",
    "            \"name\": \"Learning Rate Scheduling\"\n",
    "        }\n",
    "        return [optimizer], [scheduler]\n",
    "# pytorch lightning module of EEGNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cdbdf0",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6560c5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs = 5, accelerator=\"auto\") #progress_bar_refresh_rate=20, update every 20 batch to reduce colab crasjh  #gpus=1\n",
    "trainer.fit(model) #1000 epochs default"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
